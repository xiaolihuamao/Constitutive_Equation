{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这个文件用于绘制通过PINN模型对CVAE生成的多模态解进行绘图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import random, genfromtxt\n",
    "from IPython.display import display\n",
    "from matplotlib import rc\n",
    "from matplotlib.pyplot import figure\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.ticker as mticker\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.font_manager as fm\n",
    "import matplotlib as mpl   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/share/users/fxy/work/Constitutive_Equation/MFNN/singlefluid\n",
      "Using GPU: NVIDIA GeForce RTX 3090\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "device=torch.device(\"cuda\")\n",
    "# 检查是否有可用的 GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # 使用 GPU\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # 使用 CPU\n",
    "    print(\"No GPU available, using CPU instead.\")\n",
    "device=torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "系统已安装 Arial 字体\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "# 检查系统是否安装了 Arial 字体\n",
    "def is_arial_available():\n",
    "    for font in fm.fontManager.ttflist:\n",
    "        if 'Arial' in font.name:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# 如果系统没有 Arial 字体，加载用户自定义字体\n",
    "if not is_arial_available():\n",
    "    user_font_path = os.path.expanduser('~/.local/share/fonts/ARIAL.TTF')\n",
    "    if os.path.exists(user_font_path):\n",
    "        # 添加用户字体到 Matplotlib 的字体管理器\n",
    "        fm.fontManager.addfont(user_font_path)\n",
    "        # 设置 Matplotlib 使用该字体\n",
    "        plt.rcParams['font.family'] = 'sans-serif'\n",
    "        plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans']\n",
    "        print(\"已加载用户自定义 Arial 字体\")\n",
    "    else:\n",
    "        print(\"未找到用户自定义 Arial 字体文件\")\n",
    "else:\n",
    "    print(\"系统已安装 Arial 字体\")\n",
    "\n",
    "# 重置 Matplotlib 的全局设置\n",
    "mpl.rcParams.update(mpl.rcParamsDefault)\n",
    "plt.rcParams['font.family'] = ['Arial']  # 设置字体为 Arial\n",
    "plt.rcParams['font.size'] = 18  # 设置全局字体大小\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载生成的标签数据,数据形状为: (100, 6)\n",
      "前5行数据为:\n",
      " [[19.24592718 52.19185764 79.01475178 10.93694987 20.16281711 28.81637722]\n",
      " [20.90636851 52.32290053 79.24331445  9.34296138 20.44041671 30.77089764]\n",
      " [19.90967153 52.34865365 78.65866532 10.72613774 19.16339813 28.16293688]\n",
      " [20.41629038 53.5296622  79.73968513  9.81620437 19.83479733 30.57848702]\n",
      " [19.55662918 52.95986368 79.12772338 10.3691067  19.775179   29.90009443]]\n"
     ]
    }
   ],
   "source": [
    "# 读取生成的标签数据并转换为numpy数组\n",
    "generated_labels = pd.read_excel('Data/generated_labels.xlsx', sheet_name='Generated Labels')\n",
    "label_columns = ['Mn1', 'Mn2', 'Mn3', 'Mn11', 'Mn22', 'Mn33']\n",
    "generated_labels_array = generated_labels[label_columns].to_numpy()\n",
    "print(\"成功加载生成的标签数据,数据形状为:\", generated_labels_array.shape)\n",
    "print(\"前5行数据为:\\n\", generated_labels_array[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算哈达玛积\n",
    "def add_hadamard_features(X):\n",
    "    \"\"\"\n",
    "    计算哈达玛积并拼接新特征。\n",
    "\n",
    "    参数:\n",
    "        X (numpy.ndarray): 输入的特征矩阵，形状为 (n_samples, n_features)。\n",
    "                          列顺序必须为 ['AngFreq', 'Mn1', 'Mn2', 'Mn3', 'Mn11', 'Mn22', 'Mn33']。\n",
    "\n",
    "    返回:\n",
    "        numpy.ndarray: 处理后的特征矩阵，形状为 (n_samples, 4)。\n",
    "                      列顺序为 ['AngFreq', 'Mn1*Mn11', 'Mn2*Mn22', 'Mn3*Mn33']。\n",
    "    \"\"\"\n",
    "   # 计算哈达玛积\n",
    "    Mn1_Mn11 = X[:, 1] * X[:, 4]  # Mn1 * Mn11\n",
    "    Mn2_Mn22 = X[:, 2] * X[:, 5]  # Mn2 * Mn22\n",
    "    Mn3_Mn33 = X[:, 3] * X[:, 6]  # Mn3 * Mn33\n",
    "\n",
    "    # 计算交叉相乘\n",
    "    feature_1_2 = Mn1_Mn11 * Mn2_Mn22  # (Mn1 * Mn11) * (Mn2 * Mn22)\n",
    "    feature_1_3 = Mn1_Mn11 * Mn3_Mn33  # (Mn1 * Mn11) * (Mn3 * Mn33)\n",
    "    feature_2_3 = Mn2_Mn22 * Mn3_Mn33  # (Mn2 * Mn22) * (Mn3 * Mn33)\n",
    "    feature_1_2_3=Mn1_Mn11*Mn2_Mn22*Mn3_Mn33\n",
    "    # 拼接新特征\n",
    "    X_new = np.column_stack((\n",
    "        X[:, 0],  # AngFreq\n",
    "        Mn1_Mn11,\n",
    "        Mn2_Mn22,\n",
    "        Mn3_Mn33,\n",
    "        feature_1_2,\n",
    "        feature_1_3,\n",
    "        feature_2_3\n",
    "    ))\n",
    "    return X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#获取原始数据（主要是获取原本训练时的归一化参数，理论上不应该这样处理，另外一个是获取b4对应的频率-损耗角序列）\n",
    "\n",
    "# 设置数据类型\n",
    "DTYPE = torch.float32\n",
    "\n",
    "# 设置随机种子\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "df = {}\n",
    "# 读取 Excel 文件中的所有 sheet\n",
    "url_hf = 'Data/DifferComponent_data.xlsm'\n",
    "df_HF = pd.read_excel(url_hf, sheet_name=None)\n",
    "# 初始化训练集和验证集\n",
    "train_set = pd.DataFrame()  # 用于存储训练集\n",
    "valid_set = pd.DataFrame()  # 用于存储验证集\n",
    "\n",
    "# 遍历所有 sheet\n",
    "for sheet_name, df in df_HF.items():\n",
    "    # 移除包含 NaN 的行\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # 根据 sheet_name 划分数据集\n",
    "    if sheet_name == 'b4': \n",
    "        valid_set = df  \n",
    "    if sheet_name!='b1':\n",
    "        train_set = pd.concat([train_set, df], ignore_index=True)  # 其他 sheet 拼接为训练集\n",
    "\n",
    "# 提取目标变量\n",
    "y1_train = train_set['lossF'].to_numpy()\n",
    "y1_valid=valid_set['lossF'].to_numpy()\n",
    "# 提取特征变量\n",
    "feature_columns = ['AngFreq', 'Mn1', 'Mn2', 'Mn3',  \n",
    "                   'Mn11', 'Mn22', 'Mn33']\n",
    "X_train = train_set[feature_columns].to_numpy()\n",
    "\n",
    "X_train = add_hadamard_features(X_train)\n",
    "# 对频率特征进行对数化处理\n",
    "X_train[:, 0] = np.log10(X_train[:, 0])  # AngFreq 是第一个特征\n",
    "\n",
    "# 初始化 MinMaxScaler\n",
    "x_scaler = MinMaxScaler()  # 用于特征 X 的归一化\n",
    "y_scaler = MinMaxScaler()  # 用于目标 y 的归一化\n",
    "\n",
    "# 对 X_train 进行归一化\n",
    "X_train_normalized = x_scaler.fit_transform(X_train)\n",
    "\n",
    "# 对 y_train 进行归一化\n",
    "# 注意：y_train 需要 reshape 为二维数组，因为 MinMaxScaler 接受二维输入\n",
    "# y_train_normalized = y_scaler.fit_transform(y1_train.reshape(-1, 1))\n",
    "\n",
    "# # 将归一化后的训练数据转换为 PyTorch 张量\n",
    "# X_data_HF = torch.tensor(X_train_normalized, dtype=torch.float32)\n",
    "# y_data_HF = torch.tensor(y_train_normalized, dtype=torch.float32)\n",
    "\n",
    "# 创建存储100个验证集的列表\n",
    "valid_sets = []\n",
    "\n",
    "# 遍历generated_labels_array的行数次生成不同的测试集\n",
    "for i in range(len(generated_labels_array)):\n",
    "    # 复制原始valid_set\n",
    "    temp_valid_set = valid_set.copy()\n",
    "    \n",
    "    # 替换对应列的值为generated_labels_array中的第i行\n",
    "    temp_valid_set['Mn1'] = generated_labels_array[i,0]\n",
    "    temp_valid_set['Mn2'] = generated_labels_array[i,1] \n",
    "    temp_valid_set['Mn3'] = generated_labels_array[i,2]\n",
    "    temp_valid_set['Mn11'] = generated_labels_array[i,3]\n",
    "    temp_valid_set['Mn22'] = generated_labels_array[i,4]\n",
    "    temp_valid_set['Mn33'] = generated_labels_array[i,5]\n",
    "\n",
    "    # 提取特征变量\n",
    "    X_valid = temp_valid_set[feature_columns].to_numpy()\n",
    "    \n",
    "    # 添加哈达玛积特征\n",
    "    X_valid = add_hadamard_features(X_valid)\n",
    "    \n",
    "    # 对频率特征进行对数化处理\n",
    "    X_valid[:, 0] = np.log10(X_valid[:, 0])\n",
    "    \n",
    "    # 使用已有的scaler进行归一化\n",
    "    X_valid_normalized = x_scaler.transform(X_valid)\n",
    "    \n",
    "    # 添加到列表中\n",
    "    valid_sets.append(X_valid_normalized)\n",
    "\n",
    "# 定义模型维度\n",
    "# 将所有验证集转换为PyTorch张量\n",
    "valid_sets_tensor = [torch.tensor(valid_set, dtype=torch.float32) for valid_set in valid_sets]\n",
    "\n",
    "# 创建对应的y_valid张量列表\n",
    "y_valid_list = []\n",
    "for i in range(len(generated_labels_array)):\n",
    "    # 使用模型预测生成y_valid\n",
    "    y_valid_list.append(y1_valid)\n",
    "\n",
    "# 将所有验证集数据保存在列表中\n",
    "X_valid_list = valid_sets_tensor  # 直接使用已有的tensor列表\n",
    "y_valid_list = [torch.tensor(y, dtype=torch.float32) for y in y_valid_list]  # 将numpy数组转换为tensor并存入列表\n",
    "\n",
    "in_dim, out_dim = 7, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 神经网络的类\n",
    "class PINN_NeuralNet(nn.Module):\n",
    "    \"\"\" Set basic architecture of the PINN model.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_dim=0,\n",
    "                 output_dim=1,  # 默认输出维度为1\n",
    "                 num_hidden_layers=4, \n",
    "                 num_neurons_per_layer=20,\n",
    "                 activation='tanh',\n",
    "                 kernel_initializer='glorot_normal',\n",
    "                 **kwargs):\n",
    "        super(PINN_NeuralNet, self).__init__()\n",
    "\n",
    "        self.num_hidden_layers = num_hidden_layers\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # 添加输入层\n",
    "        self.input_layer = nn.Linear(input_dim, num_neurons_per_layer)\n",
    "        \n",
    "        # 添加其他隐藏层\n",
    "        self.hidden_layers = nn.ModuleList()\n",
    "        for _ in range(num_hidden_layers):\n",
    "            self.hidden_layers.append(nn.Linear(num_neurons_per_layer, num_neurons_per_layer))\n",
    "        \n",
    "        # 添加输出层\n",
    "        self.out = nn.Linear(num_neurons_per_layer, output_dim)\n",
    "        # 设置激活函数\n",
    "        if activation == 'tanh':\n",
    "            self.activation = torch.tanh\n",
    "        elif activation == 'relu':\n",
    "            self.activation = F.relu6\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = torch.sigmoid\n",
    "        elif activation == 'linear':\n",
    "            self.activation = None\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported activation function\")\n",
    "        \n",
    "        # 初始化权重\n",
    "        if kernel_initializer == 'glorot_normal':\n",
    "            nn.init.xavier_normal_(self.input_layer.weight)\n",
    "            for hidden_layer in self.hidden_layers:\n",
    "                nn.init.xavier_normal_(hidden_layer.weight)\n",
    "            nn.init.xavier_normal_(self.out.weight)\n",
    "        elif kernel_initializer == 'glorot_uniform':\n",
    "            nn.init.xavier_uniform_(self.input_layer.weight)\n",
    "            for hidden_layer in self.hidden_layers:\n",
    "                nn.init.xavier_uniform_(hidden_layer.weight)\n",
    "            nn.init.xavier_uniform_(self.out.weight)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported kernel initializer\")\n",
    "\n",
    "    def forward(self, X):\n",
    "\n",
    "        # 进入输入层\n",
    "        Z = self.input_layer(X)\n",
    "        \n",
    "        # 通过隐藏层\n",
    "        for hidden_layer in self.hidden_layers:\n",
    "            Z = hidden_layer(Z)\n",
    "            if self.activation is not None:\n",
    "                Z = self.activation(Z)\n",
    "        # 通过输出层输出\n",
    "        Z = self.out(Z)\n",
    "        \n",
    "        return Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/users/fxy/.conda/envs/PINN/lib/python3.9/site-packages/torch/nn/init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n",
      "<ipython-input-18-83221015aaf7>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict_l = torch.load(\"model/PFGs/model_l_pinn_pfgs\", map_location=device)\n",
      "<ipython-input-18-83221015aaf7>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  state_dict_nl = torch.load(\"model/PFGs/model_nl_pinn_pfgs\", map_location=device)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Expected state_dict to be dict-like, got <class '__main__.PINN_NeuralNet'>.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m state_dict_nl \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel/PFGs/model_nl_pinn_pfgs\u001b[39m\u001b[38;5;124m\"\u001b[39m, map_location\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# 将参数加载到模型实例中\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[43mmodel_l_pinn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_dict_l\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m model_nl_pinn\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict_nl)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# 组合成列表（可选）\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/PINN/lib/python3.9/site-packages/torch/nn/modules/module.py:2516\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Copy parameters and buffers from :attr:`state_dict` into this module and its descendants.\u001b[39;00m\n\u001b[1;32m   2480\u001b[0m \n\u001b[1;32m   2481\u001b[0m \u001b[38;5;124;03mIf :attr:`strict` is ``True``, then\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2513\u001b[0m \u001b[38;5;124;03m    ``RuntimeError``.\u001b[39;00m\n\u001b[1;32m   2514\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(state_dict, Mapping):\n\u001b[0;32m-> 2516\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   2517\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected state_dict to be dict-like, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(state_dict)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2518\u001b[0m     )\n\u001b[1;32m   2520\u001b[0m missing_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2521\u001b[0m unexpected_keys: List[\u001b[38;5;28mstr\u001b[39m] \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mTypeError\u001b[0m: Expected state_dict to be dict-like, got <class '__main__.PINN_NeuralNet'>."
     ]
    }
   ],
   "source": [
    "\n",
    "X_data_valid_list= X_valid_list\n",
    "y_data_valid_list=y_valid_list\n",
    "# 初始化空模型实例\n",
    "model_l_pinn = PINN_NeuralNet().to(device)  # 线性模型实例\n",
    "model_nl_pinn = PINN_NeuralNet().to(device) # 非线性模型实例\n",
    "\n",
    "# 加载参数（仅 state_dict）\n",
    "state_dict_l = torch.load(\"model/PFGs/model_l_pinn_pfgs\", map_location=device)\n",
    "state_dict_nl = torch.load(\"model/PFGs/model_nl_pinn_pfgs\", map_location=device)\n",
    "\n",
    "# 将参数加载到模型实例中\n",
    "model_l_pinn.load_state_dict(state_dict_l)\n",
    "model_nl_pinn.load_state_dict(state_dict_nl)\n",
    "\n",
    "# 组合成列表（可选）\n",
    "model_pinn = [model_l_pinn, model_nl_pinn]\n",
    "# 创建图表\n",
    "fig, ax = plt.subplots(figsize=(8, 6), dpi=600)\n",
    "# 定义模型名称和颜色\n",
    "model_names = ['DNN', 'PINN', 'PINN-HP', 'PINN-AFF']\n",
    "model_colors = ['tab:blue', 'tab:orange', 'tab:green', 'tab:purple']\n",
    "\n",
    "# 对验证集进行预测并绘制\n",
    "for model, name, color in zip([model_pinn], model_names, model_colors):\n",
    "    # 模型预测\n",
    "    y_MF_valid = model[1](X_data_valid_list[0]) + model[0](X_data_valid_list[0])\n",
    "    y_MF_valid_denorm = y_scaler.inverse_transform(y_MF_valid.detach().numpy())\n",
    "    \n",
    "    # 绘制验证集的拟合曲线\n",
    "    ax.plot(10**X_valid[:, 0], y_MF_valid_denorm, color=color, label=f'{name}', linewidth=3)  # 设置曲线宽度\n",
    "\n",
    "\n",
    "# 绘制验证集的真实值（空心圆）\n",
    "y1_valid_denorm = y_scaler.inverse_transform(y_data_valid_list[0].numpy())\n",
    "ax.scatter(10**X_valid[:, 0], y1_valid_denorm, color='black', marker='o', s=50, label='Test')  # 设置点图大小\n",
    "\n",
    "# 设置图表属性\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel('tan$\\delta$')  # 设置纵坐标名称字体大小\n",
    "ax.set_xlabel('$\\omega$ (rad/s)')  # 设置横坐标名称字体大小\n",
    "ax.legend(loc='upper left',frameon=False,fontsize=18,ncol=2)  # 设置图例位置和大小\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PINN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
