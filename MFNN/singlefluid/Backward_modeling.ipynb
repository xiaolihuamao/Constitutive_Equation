{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 尝试反向建模生成制备参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import random, genfromtxt\n",
    "from IPython.display import display\n",
    "from matplotlib import rc\n",
    "from matplotlib.pyplot import figure\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.ticker as mticker\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_error, r2_score\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/redfu/work/Constitutive_Equation/MFNN/singlefluid\n",
      "Using GPU: NVIDIA GeForce RTX 2080 Ti\n"
     ]
    }
   ],
   "source": [
    "current_directory = os.getcwd()\n",
    "print(current_directory)\n",
    "device=torch.device(\"cuda\")\n",
    "# 检查是否有可用的 GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # 使用 GPU\n",
    "    print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")  # 使用 CPU\n",
    "    print(\"No GPU available, using CPU instead.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded inputs shape: torch.Size([6, 61, 2])\n",
      "Labels shape: torch.Size([6, 6])\n",
      "Sequence lengths: tensor([36, 35, 61, 36, 41, 42])\n"
     ]
    }
   ],
   "source": [
    "# 设置随机种子\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "\n",
    "# 读取 Excel 文件中的所有 sheet\n",
    "url_hf = 'Data/DifferComponent_data.xlsm'\n",
    "df_HF = pd.read_excel(url_hf, sheet_name=None)\n",
    "\n",
    "# 初始化存储输入和标签的列表\n",
    "inputs = []  # 存储每个 sheet 的 AngFreq 和 lossF\n",
    "labels = []  # 存储每个 sheet 的 Mn1, Mn2, Mn3, Mn11, Mn22, Mn33\n",
    "sequence_lengths = []  # 存储每个 sheet 的长度\n",
    "\n",
    "# 遍历所有 sheet\n",
    "for sheet_name, df in df_HF.items():\n",
    "    # 移除包含 NaN 的行\n",
    "    df = df.dropna()\n",
    "    if sheet_name == 's':\n",
    "        continue\n",
    "    # 提取 AngFreq 和 lossF 作为输入特征\n",
    "    ang_freq = df['AngFreq'].values.astype(np.float32)\n",
    "    loss_f = df['lossF'].values.astype(np.float32)\n",
    "    \n",
    "    # 对 AngFreq 进行对数化处理\n",
    "    ang_freq_log = np.log10(ang_freq)  # 使用 log10，也可以使用自然对数 np.log\n",
    "    \n",
    "    # 合并为 (N, 2) 的数组\n",
    "    input_feature = np.column_stack((ang_freq_log, loss_f))\n",
    "    \n",
    "    # 提取 Mn1, Mn2, Mn3, Mn11, Mn22, Mn33 作为标签（只取第一行）\n",
    "    label = df[['Mn1', 'Mn2', 'Mn3', 'Mn11', 'Mn22', 'Mn33']].iloc[0].values.astype(np.float32)\n",
    "    \n",
    "    # 保存输入和标签\n",
    "    inputs.append(input_feature)\n",
    "    labels.append(label)\n",
    "    sequence_lengths.append(len(input_feature))  # 记录序列长度\n",
    "\n",
    "# 对输入特征进行归一化\n",
    "input_scaler = MinMaxScaler()\n",
    "inputs_normalized = [input_scaler.fit_transform(x) for x in inputs]  # 对每个序列单独归一化\n",
    "\n",
    "# 对标签进行归一化\n",
    "label_scaler = MinMaxScaler()\n",
    "labels_normalized = label_scaler.fit_transform(np.array(labels))  # 对所有标签一起归一化\n",
    "\n",
    "# 将输入和标签转换为 PyTorch 张量\n",
    "inputs_tensor = [torch.tensor(x, dtype=torch.float32) for x in inputs_normalized]\n",
    "labels_tensor = torch.tensor(labels_normalized, dtype=torch.float32)\n",
    "\n",
    "# 对变长序列进行填充\n",
    "inputs_padded = pad_sequence(inputs_tensor, batch_first=True)  # 填充为相同长度\n",
    "\n",
    "# 记录每个序列的实际长度\n",
    "sequence_lengths = torch.tensor(sequence_lengths, dtype=torch.long)\n",
    "\n",
    "# 打印填充后的输入和标签形状\n",
    "print(\"Padded inputs shape:\", inputs_padded.shape)  # (num_sheets, max_length, 2)\n",
    "print(\"Labels shape:\", labels_tensor.shape)  # (num_sheets, 6)\n",
    "print(\"Sequence lengths:\", sequence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 创建数据集\n",
    "dataset = TensorDataset(inputs_padded, labels_tensor, sequence_lengths)\n",
    "\n",
    "# 创建 DataLoader\n",
    "batch_size = 1  # 根据需求调整\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim, output_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.latent_dim = latent_dim\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # 编码器\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv1d(in_channels=input_dim, out_channels=16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Conv1d(in_channels=16, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # 潜在空间的均值和方差\n",
    "        # 这里暂时不定义 fc_mu 和 fc_logvar，因为在 forward 中动态计算\n",
    "        self.fc_mu = None\n",
    "        self.fc_logvar = None\n",
    "        \n",
    "        # 解码器\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 动态计算 max_length\n",
    "        batch_size, _, max_length = x.shape\n",
    "        \n",
    "        # 编码\n",
    "        h = self.encoder(x.permute(0, 2, 1))  # 转换为 (batch_size, 2, sequence_length)\n",
    "        \n",
    "        # 动态定义 fc_mu 和 fc_logvar\n",
    "        if self.fc_mu is None:\n",
    "            self.fc_mu = nn.Linear(h.shape[1], self.latent_dim).to(x.device)\n",
    "            self.fc_logvar = nn.Linear(h.shape[1], self.latent_dim).to(x.device)\n",
    "        \n",
    "        mu, logvar = self.fc_mu(h), self.fc_logvar(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        \n",
    "        # 解码\n",
    "        return self.decoder(z), mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.4003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101, Loss: 0.9940\n",
      "Epoch 201, Loss: 0.6837\n",
      "Epoch 301, Loss: 2.0237\n",
      "Epoch 401, Loss: 0.3791\n",
      "Epoch 501, Loss: 0.5744\n",
      "Epoch 601, Loss: 0.5755\n",
      "Epoch 701, Loss: 0.5737\n",
      "Epoch 801, Loss: 0.8557\n",
      "Epoch 901, Loss: 1.8121\n"
     ]
    }
   ],
   "source": [
    "# 初始化模型\n",
    "input_dim = 2  # AngFreq 和 lossF\n",
    "hidden_dim = 4\n",
    "latent_dim = 10\n",
    "output_dim = 6  # Mn1, Mn2, Mn3, Mn11, Mn22, Mn33\n",
    "model = VAE(input_dim, hidden_dim, latent_dim, output_dim)\n",
    "\n",
    "# 定义优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 定义损失函数\n",
    "def loss_function(recon_x, x, mu, logvar):\n",
    "    # 重构损失\n",
    "    BCE = nn.functional.mse_loss(recon_x, x, reduction='sum')\n",
    "    # KL散度\n",
    "    KLD = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD\n",
    "\n",
    "# 训练\n",
    "for epoch in range(1000):  # 假设训练 10 个 epoch\n",
    "    for batch in dataloader:\n",
    "        inputs, labels, lengths = batch\n",
    "        optimizer.zero_grad()\n",
    "        recon_batch, mu, logvar = model(inputs)\n",
    "        loss = loss_function(recon_batch, labels, mu, logvar)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch%100==0:\n",
    "        print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output for inputs (after training):\n",
      "[[3.6120186e+01 9.8328957e+01 3.7598652e+01 2.1636653e-01 3.1900376e-01\n",
      "  6.6812620e-02]]\n"
     ]
    }
   ],
   "source": [
    "# 训练结束后，打印模型对 inputs 的输出并反归一化\n",
    "with torch.no_grad():  # 禁用梯度计算\n",
    "    model.eval()  # 将模型设置为评估模式\n",
    "    recon_batch, mu, logvar = model(inputs)  # 获取模型输出\n",
    "    \n",
    "    # 将模型输出反归一化\n",
    "    recon_batch_np = recon_batch.detach().numpy()  # 转换为 NumPy 数组\n",
    "    recon_batch_original = label_scaler.inverse_transform(recon_batch_np)  # 反归一化\n",
    "    \n",
    "    print(\"Model output for inputs (after training):\")\n",
    "    print(recon_batch_original)  # 打印反归一化后的模型输出"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PINN",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
